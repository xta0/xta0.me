I"ѝ<h3 id="motivation">Motivation</h3>

<p>PyTorch是Facebook开源的一套Deep Learning的框架，由于它有基于Python的Frontend API，因此非常容易上手，对Researcher也非常友好。我目前对PyTorch的理解是它是具有自动求导功能的Numpy，当然PyTorch比Numpy肯定要强大的多。</p>

<h3 id="linear-regression">Linear Regression</h3>

<p>我们使用的例子是一个很简单的<a href="https://xta0.me/2017/09/20/Machine-Learning-2.html">线性回归模型</a>，假设我们有一组观测数据如下</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t_y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">14.0</span><span class="p">,</span> <span class="mf">15.0</span><span class="p">,</span> <span class="mf">28.0</span><span class="p">,</span> <span class="mf">11.0</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">,</span> <span class="mf">3.0</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0</span><span class="p">,</span> <span class="mf">6.0</span><span class="p">,</span> <span class="mf">13.0</span><span class="p">,</span> <span class="mf">21.0</span><span class="p">]</span>
<span class="n">t_x</span> <span class="o">=</span> <span class="p">[</span><span class="mf">35.7</span><span class="p">,</span> <span class="mf">55.9</span><span class="p">,</span> <span class="mf">58.2</span><span class="p">,</span> <span class="mf">81.9</span><span class="p">,</span> <span class="mf">56.3</span><span class="p">,</span> <span class="mf">48.9</span><span class="p">,</span> <span class="mf">33.9</span><span class="p">,</span> <span class="mf">21.8</span><span class="p">,</span> <span class="mf">48.4</span><span class="p">,</span> <span class="mf">60.4</span><span class="p">,</span> <span class="mf">68.4</span><span class="p">]</span>
</code></pre></div></div>
<p>我们先观察一下数据的分布</p>

<p><img src="/assets/images/2019/06/pytorch-lr-0.png" width="60%" /></p>

<p>图中可以看出，我们的数据只有一个feature，我们的目标是找到一个线性模型，使下面等式成立</p>

<script type="math/tex; mode=display">t_y = \omega \times t_x + b</script>

<p>实际上就是对上述的离散点进行线性拟合。我们首先来创建两个tensor</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>

<span class="n">t_y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t_y</span><span class="p">)</span>
<span class="n">t_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">t_x</span><span class="p">)</span>
</code></pre></div></div>
<p>接下来我们来定义我们的model和Loss函数</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">w</span><span class="o">*</span><span class="n">t_x</span> <span class="o">+</span> <span class="n">b</span>

<span class="k">def</span> <span class="nf">loss_fn</span><span class="p">(</span><span class="n">t_y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">):</span>
    <span class="n">squared_diff</span> <span class="o">=</span> <span class="p">(</span><span class="n">t_y</span><span class="o">-</span><span class="n">t_p</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">return</span> <span class="n">squared_diff</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</code></pre></div></div>
<p>我们可以先随机生成一组数据，跑一下</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">w</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">t_y</span><span class="p">,</span><span class="n">t_p</span><span class="p">)</span> <span class="o">//</span><span class="n">tensor</span><span class="p">(</span><span class="mf">2478.7595</span><span class="p">)</span>
</code></pre></div></div>
<p>可以看到loss非常大，这样在我们的意料之中，因为我们还没有对model进行训练，接下来我们要做的就是想办法来训练我们的model。由前面机器学习的文章可知，我们需要找到$\omega$，使loss函数的值最小，因此我们需要首先建立loss函数和$\omega$的关系，然后用梯度下降法找到使loss函数的收敛的极小值点，即可得到我们想要的$\omega$的值。$b$的计算同理。以$\omega$为例，回忆梯度下降的公式为</p>

<script type="math/tex; mode=display">w := w - \alpha \times \frac {dL}{d\omega}</script>

<p>其中$\alpha$为Learning Rate，loss函数对$\omega$的导数，我们可以使用链式求导法则来得到</p>

<script type="math/tex; mode=display">\frac {dL}{d\omega} = \frac {dL}{d\hat y} \times \frac {d\hat y}{d\omega}</script>

<p>对应到代码中，我们需要定义一个求导函数<code class="highlighter-rouge">grad_fn</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">dloss_fn</span><span class="p">(</span><span class="n">t_y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">):</span>
    <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="p">(</span><span class="n">t_y</span><span class="o">-</span><span class="n">t_p</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">dmodel_dw</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">t_x</span>

<span class="k">def</span> <span class="nf">dmodel_db</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">1.0</span>

<span class="k">def</span> <span class="nf">grad_fn</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">t_y</span><span class="p">,</span><span class="n">t_p</span><span class="p">):</span>
    <span class="n">dw</span> <span class="o">=</span> <span class="n">dloss_fn</span><span class="p">(</span><span class="n">t_y</span><span class="p">,</span><span class="n">t_p</span><span class="p">)</span> <span class="o">*</span> <span class="n">dmodel_dw</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="n">dloss_fn</span><span class="p">(</span><span class="n">t_y</span><span class="p">,</span><span class="n">t_p</span><span class="p">)</span> <span class="o">*</span> <span class="n">dmodel_db</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">dw</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">db</span><span class="o">.</span><span class="n">mean</span><span class="p">()])</span> 
</code></pre></div></div>

<p>有了梯度函数后，我们便可以使用梯度下降接法来训练我们的model了</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">learning_rate</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
    <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"loss: "</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>
    <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">t_y</span><span class="p">,</span><span class="n">t_p</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">b</span> <span class="o">=</span> <span class="n">b</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">t_x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">t_y</span><span class="p">,</span><span class="n">t_p</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"loss: "</span><span class="p">,</span><span class="n">loss</span><span class="p">)</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">)</span>
</code></pre></div></div>
<p>上面我们先执行了一次forward pass，得到了loss，然后进行了一次backward pass，得到了新的$\omega$和$b$，接着我们又进行了一次forward，又得到了一个新的loss值。我们可以猜想，loss值应该会变小，我们训练一次观察一下结果</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> 
<span class="n">w</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> 
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span> 
<span class="n">x</span><span class="o">=</span><span class="n">t_x</span><span class="p">,</span> 
<span class="n">y</span><span class="o">=</span><span class="n">t_y</span><span class="p">)</span>

<span class="n">loss</span><span class="p">:</span>  <span class="mf">1763.8846435546875</span>
<span class="n">loss</span><span class="p">:</span>  <span class="mf">5802484.5</span>
</code></pre></div></div>
<p>我们发现loss的值没有变小，反而变大了。回忆前面机器学习的文章，出现这种情况，有几种可能，比如<code class="highlighter-rouge">learning_rate</code>的值过大，或者没有对输入数据进行normalization。我们先试着改变步长</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> 
<span class="n">w</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> 
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span> 
<span class="n">x</span><span class="o">=</span><span class="n">t_x</span><span class="p">,</span> 
<span class="n">y</span><span class="o">=</span><span class="n">t_y</span><span class="p">)</span>
<span class="c1">#----------------------------
</span><span class="n">loss</span><span class="p">:</span>  <span class="mf">1763.8846435546875</span>
<span class="n">loss</span><span class="p">:</span>  <span class="mf">323.0905456542969</span>
</code></pre></div></div>
<p>我们发现loss的值变小了，符合我们的预期。接着我们尝试对输入数据进行normalization</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">t_xn</span> <span class="o">=</span> <span class="n">t_x</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">train</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> 
<span class="n">w</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">1.0</span><span class="p">),</span> 
<span class="n">b</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">),</span> 
<span class="n">x</span><span class="o">=</span><span class="n">t_xn</span><span class="p">,</span> 
<span class="n">y</span><span class="o">=</span><span class="n">t_y</span><span class="p">)</span>
<span class="c1">#----------------------------
</span><span class="n">loss</span><span class="p">:</span>  <span class="mf">80.36434173583984</span>
<span class="n">loss</span><span class="p">:</span>  <span class="mf">37.57491683959961</span>
</code></pre></div></div>
<blockquote>
  <p>这里我们可以使用更复杂的normalization方法，这里为了简单起见，直接领输入数据乘以0.1</p>
</blockquote>

<p>通过一系列操作，loss已经可以收敛了，这说明我们的梯度下降可以正常工作，接下来我们便可以正式训练了，我们对上面代码稍微重构一下</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_loop</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">)</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad_fn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">w</span><span class="p">,</span><span class="n">b</span><span class="p">,</span><span class="n">y</span><span class="p">,</span><span class="n">t_p</span><span class="p">)</span>
        <span class="n">params</span> <span class="o">=</span> <span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grad</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch: {epoch}, Loss: {float(loss)}'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>

<span class="n">param</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> 
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> 
<span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">]),</span> 
<span class="n">x</span> <span class="o">=</span> <span class="n">t_x</span><span class="p">,</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">t_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"w,b"</span><span class="p">,</span><span class="nb">float</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">float</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
<span class="c1">#----------------------------
</span><span class="n">Epoch</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">80.36434173583984</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span>  <span class="mf">37.57491683959961</span>
<span class="o">...</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">4999</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.927647352218628</span>
<span class="n">Epoch</span><span class="p">:</span> <span class="mi">5000</span><span class="p">,</span> <span class="n">Loss</span><span class="p">:</span> <span class="mf">2.927647590637207</span>
<span class="c1">#----------------------------
</span><span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="mf">5.367083549499512</span> <span class="o">-</span><span class="mf">17.301189422607422</span>
</code></pre></div></div>
<p>我们循环了5000次，loss收敛在<code class="highlighter-rouge">2.927647</code>左右，不再下降，此时我们可以认为得到的$\omega$和$b$是我们最终想要的，为了更直观的理解，我们将训练得到模型画出来，其中黄色的点为我们原始离散数据点，蓝色的线是我们训练好的model，即拟合出来的曲线</p>

<p><img src="/assets/images/2019/06/pytorch-lr-1.png" /></p>

<h3 id="autograd">Autograd</h3>

<p>上述代码并没有什么特别的地方，我们手动的实现了对$\omega$和$b$的求导，但由于上面的model太过简单，因此难度不大。但是对于复杂的model，比如CNN的model，涉及到大量的待学习的参数，如果纯用手动求导的方式则会非常复杂，且容易出错。正如我前面所说，PyTorch强大的地方在于不论model多复杂，只要它满足可微分的条件，PyTorch便可以自动帮我们完成求导的计算，即所谓的<strong>autograd</strong>。</p>

<p>简单来说，对所有的Model，我们都可以用一个<a href="https://xta0.me/2018/01/02/Deep-Learning-1.html">Computational Graph</a>来表示，Graph中的每个节点代表一个运算函数</p>

<p><img src="/assets/images/2018/01/dp-w2-1.png" /></p>

<p>如上图中的<code class="highlighter-rouge">a</code>,<code class="highlighter-rouge">b</code>,<code class="highlighter-rouge">c</code>为叶子节点，在执行forward pass的时候，PyTorch会记住(record)非叶子节点上的函数，保存在该节点对应的tensor中，这样当做backward pass的时候便可以很方便的使用链式求导快速计算出叶子节点的导数值。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">5.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">8.0</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>
<p>这里我们定义了<code class="highlighter-rouge">a</code>,<code class="highlighter-rouge">b</code>,<code class="highlighter-rouge">c</code>三个tensor，并且告诉PyTorch需要对它们进行求导，接下来我们建立Computation Graph</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">u</span> <span class="o">=</span> <span class="n">b</span><span class="o">*</span><span class="n">c</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">a</span><span class="o">+</span><span class="n">u</span>
<span class="n">j</span> <span class="o">=</span> <span class="mi">3</span><span class="o">*</span><span class="n">v</span>
</code></pre></div></div>

<p>接下来我们可以验证下每个节点上都有什么信息</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">&gt;&gt;&gt;</span> <span class="n">u</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">40.</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">j</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">129.</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">MulBackward0</span><span class="o">&gt;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">v</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">43.</span><span class="p">,</span> <span class="n">grad_fn</span><span class="o">=&lt;</span><span class="n">AddBackward0</span><span class="o">&gt;</span><span class="p">)</span>
</code></pre></div></div>
<p>可以看到每个节点上的值都被求出来了，也就是PyTorch中所谓的eager mode，另外这几个tensor上面都有<code class="highlighter-rouge">grad_fn</code>，用来做autograd。上面的图中，<code class="highlighter-rouge">j</code>是最终节点，我们可以让<code class="highlighter-rouge">j</code>做<code class="highlighter-rouge">backward()</code>，则它会触发<code class="highlighter-rouge">u</code>和<code class="highlighter-rouge">v</code>做反向求导，而求导的结果会存放在<code class="highlighter-rouge">a</code>,<code class="highlighter-rouge">b</code>,<code class="highlighter-rouge">c</code>上</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">j</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span><span class="o">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">3.</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span><span class="o">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">24.</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span><span class="o">.</span><span class="n">grad</span>
<span class="n">tensor</span><span class="p">(</span><span class="mf">15.</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">u</span><span class="o">.</span><span class="n">grad</span> <span class="c1">#none
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">v</span><span class="o">.</span><span class="n">grad</span> <span class="c1">#none
</span><span class="o">&gt;&gt;&gt;</span> <span class="n">j</span><span class="o">.</span><span class="n">grad</span> <span class="c1">#none
</span></code></pre></div></div>
<p>上面代码中我们用<code class="highlighter-rouge">grad</code>函数查看tensor上的导数值，可见只有leaf节点才会累积求导的结果，中间节点不会保存任何中间结果。</p>

<blockquote>
  <p>关于Autograd详细的实现建议阅读源码，后面如果有时间我们可以写一篇文章专门分析</p>
</blockquote>

<p>接下来我们可以用PyTorch的autograd API重写上一节的训练代码</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use autograd
</span><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">train_loop</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">params</span><span class="o">.</span><span class="n">grad_zero</span><span class="p">()</span>
        <span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1">#autograd
</span>        <span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch: {epoch}, Loss: {float(loss)}'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>


<span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">param</span> <span class="o">=</span> <span class="n">train_loop</span><span class="p">(</span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">,</span> 
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span><span class="p">,</span> 
<span class="n">params</span> <span class="o">=</span> <span class="n">params</span><span class="p">,</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">t_xn</span><span class="p">,</span> 
<span class="n">y</span> <span class="o">=</span> <span class="n">t_y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"w,b"</span><span class="p">,</span><span class="nb">float</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="nb">float</span><span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</code></pre></div></div>

<h3 id="optimizers">Optimizers</h3>

<p>之前机器学习的文章中，我们曾提到过<a href="https://xta0.me/2017/11/17/Machine-Learning-9.html">对传统梯度下降的优化</a>，例如当数据量大时，可以使用Stochastic Gradient Descent(SGD)，另外还有些优化算法可以帮助我们加快梯度下降的收敛速度，从而减少训练时间。PyTorch内部提供了一系列优化算法的API</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="n">optim</span>
<span class="nb">dir</span><span class="p">(</span><span class="n">optim</span><span class="p">))</span>
<span class="c1">#----------------------------
</span><span class="p">[</span><span class="s">'ASGD'</span><span class="p">,</span> <span class="s">'Adadelta'</span><span class="p">,</span> <span class="s">'Adagrad'</span><span class="p">,</span> <span class="s">'Adam'</span><span class="p">,</span> <span class="s">'AdamW'</span><span class="p">,</span> <span class="s">'Adamax'</span><span class="p">,</span> 
<span class="s">'LBFGS'</span><span class="p">,</span> <span class="s">'Optimizer'</span><span class="p">,</span> <span class="s">'RMSprop'</span><span class="p">,</span> <span class="s">'Rprop'</span><span class="p">,</span> <span class="s">'SGD'</span><span class="p">,</span> <span class="s">'SparseAdam'</span><span class="p">,</span>
<span class="s">'__builtins__'</span><span class="p">,</span> <span class="s">'__cached__'</span><span class="p">,</span> <span class="s">'__doc__'</span><span class="p">,</span> <span class="s">'__file__'</span><span class="p">,</span> <span class="s">'__loader__'</span><span class="p">,</span> 
<span class="s">'__name__'</span><span class="p">,</span> <span class="s">'__package__'</span><span class="p">,</span> <span class="s">'__path__'</span><span class="p">,</span> <span class="s">'__spec__'</span><span class="p">,</span> <span class="s">'lr_scheduler'</span><span class="p">]</span>
</code></pre></div></div>
<p>Optimizer通常和autograd配合使用，因为在训练的时候它需要修改tensor的梯度值，因此Optimizer内部会retain传入tensor。使用Optimizer的方式也很简单，它提供两个API，一个是<code class="highlighter-rouge">zero_grad()</code>用于清空tensor上保存的导数值，另一个是<code class="highlighter-rouge">step()</code>用来实现具体的optimize的操作。接下来我们为上面的demo引入一个optimizaer</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">params</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">,</span><span class="mf">0.0</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-2</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">([</span><span class="n">params</span><span class="p">],</span><span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>
</code></pre></div></div>
<p>接下来我们需要在backward()执行完成后，调用<code class="highlighter-rouge">step()</code>方法来更新<code class="highlighter-rouge">params</code>中的值。另外由于optimizer提供了<code class="highlighter-rouge">zero_grad()</code>的方法，我们可以将上面手动清除导数值的方式替换成使用<code class="highlighter-rouge">zero_grad()</code>方法</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">train_loop</span><span class="p">(</span><span class="n">epochs</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>    
        <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> <span class="c1">#clear grad value on params
</span>        <span class="n">w</span><span class="p">,</span><span class="n">b</span> <span class="o">=</span> <span class="n">params</span>
        <span class="n">t_p</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_fn</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">t_p</span><span class="p">)</span>
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1">#update params
</span>        <span class="n">params</span> <span class="o">=</span> <span class="p">(</span><span class="n">params</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">params</span><span class="o">.</span><span class="n">grad</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Epoch: {epoch}, Loss: {float(loss)}'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">params</span>
</code></pre></div></div>
<p>重新训练我们model，观察结果发现和前面的结果一致。此外如果将<code class="highlighter-rouge">SGD</code>改为<code class="highlighter-rouge">Adam</code>，则loss函数的收敛速度会加快，4000次即可达到稳定状态。</p>

<h3 id="小结">小结</h3>

<p>发现到目前为止，我们已经使用PyTorch使用优化了我们的训练代码，回过头来总结一下可以发现PyTorch帮我们解决了两大块重要的工作，一个是自动求导，只需要一行backward方法即可，解放了我们的双手。另一个是提供通用的Optimizer，Optimizer的好处是将算法抽象了出来，通过直接mutate训练过程中间节点的信息达到优化参数的目的，从而不需要破坏training loop，使代码逻辑保持清晰。</p>

<h2 id="resoures">Resoures</h2>

<ul>
  <li><a href="https://xta0.me/2018/01/02/Deep-Learning-1.html">Logistic Regression as a Neural Network</a></li>
  <li><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py">AUTOGRAD: AUTOMATIC DIFFERENTIATION</a></li>
  <li><a href="https://www.youtube.com/watch?v=MswxJw-8PvE">PyTorch Autograd Explained - In-depth Tutorial</a></li>
  <li><a href="https://xta0.me/2017/11/17/Machine-Learning-9.html">Learning With Large Datasets</a></li>
</ul>
:ET