I"G<h2 id="mini-batch-gradient-descent">Mini-batch Gradient Descent</h2>

<p>假设一组训练数据$X$有$m$个样本，$X = [x^{(1)},x^{(2)},x^{(3)},…,x^{(m)}]$，每个$x^{[i]}$有n个feature，则$X$矩阵为$(n, m)$，相应的，$Y=[y^{(1)},y^{(2)},y^{(3)},…,y^{(m)}]$。对于m组数据，在training时，我们可以用vectorization的方式代替for循环，这样可以提高训练速度。</p>

<p>但是如果$m$非常大，比如5个million，那么将所有数据一起training，效率将非常低。实际上，在back prop的时候，我们不需要更新整个数据的weights和bias。我们可以将training set分成多个batch，比如将5个million切分成5000个小的batch，每个batch包含1000个训练数据。</p>

<p>使用Mini-batch gradient descent会影响training，表现为cost函数不会一直下降，而是不断变化，如下图所示</p>

<p><img src="/assets/images/2018/02/dl-ht-08.png" /></p>

<p>我们用 $X^{{t}}$ 表示一个batch的训练数据，则</p>

<ul>
  <li>当batch size为<code class="highlighter-rouge">m</code>的时候，称为<strong>Batch Gradient Descent</strong>，此时$(X^1, Y^1) = (X, Y)$</li>
  <li>当batch size为<code class="highlighter-rouge">1</code>的时候，称为<strong>Stochastic Gradient Descent</strong>，此时$(X^1, Y^1) = (x^{(1)}, y^{(1)}), …, (X^{{t}}, Y^{{t}}) = (x^{(t)}, y^{(t)})$</li>
</ul>

<p>注意，如果使用SGD，梯度下降的过程将极为noise，并不会一直沿着梯度下降最大的方向前进，也不会收敛于某个值，而是在某个区域内不断变化。</p>

<p>在实际应用中，batch size往往在<code class="highlighter-rouge">(1, m)</code>中选取。三种方式的梯度下降过程如下图所示</p>

<p><img src="/assets/images/2018/02/dl-ht-09-1.png" />
<img src="/assets/images/2018/02/dl-ht-09-2.png" /></p>

<p>小结一下</p>

<ol>
  <li>如果训练样本很小(<code class="highlighter-rouge">m&lt;2000</code>)，使用Batch Gradient Descent</li>
  <li>如果使用Mini Batch, <code class="highlighter-rouge">m</code>可选取2的阶乘，比如<code class="highlighter-rouge">64</code>, <code class="highlighter-rouge">128</code>, <code class="highlighter-rouge">256</code>, <code class="highlighter-rouge">512</code>, <code class="highlighter-rouge">1024</code></li>
  <li>注意单个batch所产生的运算量(forward和backward)是否能被加载到当前的内存中</li>
</ol>

<h2 id="gradient-descent-with-momentum">Gradient Descent with momentum</h2>

<p>Exponentially weighted averages是移动平均的意思，计算方式如下</p>

<script type="math/tex; mode=display">v_t = \beta v_{t-1} + (1-\beta) \theta_t</script>

<p>$\beta$在0和1之间，当$\beta$越大，曲线平滑，平均的样本数越多，反之曲线波动大，平均样本越少，如下图所示，分别是$\beta$为0.98，0.9和0.5时的曲线</p>

<h2 id="adam-optimization-algorithm">Adam optimization algorithm</h2>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://ruder.io/optimizing-gradient-descent/">An overview of gradient descent optimization algorithms</a></li>
</ul>
:ET