I"Œt<blockquote>
  <p>æ–‡ä¸­éƒ¨åˆ†å›¾ç‰‡æˆªå–è‡ªè¯¾ç¨‹è§†é¢‘<a href="https://www.coursera.org/learn/neural-networks-deep-learning">Nerual Networks and Deep Learning</a></p>
</blockquote>

<h3 id="notations">Notations</h3>

<ul>
  <li>$x^{(i)}$ï¼šè¡¨ç¤ºç¬¬$i$ç»„è®­ç»ƒæ ·æœ¬</li>
  <li>$x^{(i)}_j$ï¼šè¡¨ç¤ºç¬¬$i$ç»„è®­ç»ƒæ ·æœ¬çš„ç¬¬$j$ä¸ªfeature</li>
  <li>$a^{[l]}$ï¼šè¡¨ç¤ºç¬¬$l$å±‚ç¥ç»ç½‘ç»œ</li>
  <li>$a^{[l]}_i$: è¡¨ç¤ºç¬¬$l$å±‚ç¥ç»ç½‘ç»œçš„ç¬¬$i$ä¸ªèŠ‚ç‚¹</li>
  <li>$a^{[l] (m)}_i$ï¼šè¡¨ç¤ºç¬¬$m$ä¸ªè®­ç»ƒæ ·æœ¬çš„ç¬¬$l$å±‚ç¥ç»ç½‘ç»œçš„ç¬¬$i$ä¸ªèŠ‚ç‚¹</li>
</ul>

<h2 id="å•ç¥ç»ç½‘ç»œ">å•ç¥ç»ç½‘ç»œ</h2>

<p>éµå¾ªä¸Šè¿°çš„Notationï¼Œä¸€ä¸ªåªæœ‰ä¸€ç»„è®­ç»ƒæ ·æœ¬çš„$(x_1, x_2, x_3)$çš„ä¸¤å±‚ç¥ç»ç½‘ç»œå¯ç”¨ä¸‹å›¾æè¿°</p>

<p><img src="/assets/images/2018/01/dp-w3-1.png" class="md-img-center" /></p>

<p>å°†ä¸Šè¿°å¼å­ç”¨å‘é‡è¡¨ç¤ºï¼Œåˆ™å¯¹äºç»™å®šçš„è¾“å…¥$x$ï¼Œæœ‰</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& z^{[1]} = W^{[1]}x + b^{[1]} \\
& a^{[1]} = \sigma(z^{[1]}) \\ 
& z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} \\
& a^{[2]} = \sigma(z^{[2]}) 
\end{align*} %]]></script>

<p>å…¶ä¸­ï¼Œ$z^{[1]}$æ˜¯<code class="highlighter-rouge">4x1</code>ï¼Œ$W^{[1]}$æ˜¯<code class="highlighter-rouge">4x3</code>ï¼Œ$x$æ˜¯<code class="highlighter-rouge">3x1</code>ï¼Œ$b^{[1]}$æ˜¯<code class="highlighter-rouge">4x1</code>ï¼Œ$a^{[1]}$æ˜¯<code class="highlighter-rouge">4x1</code>ï¼Œ$z^{[2]}$æ˜¯<code class="highlighter-rouge">1x1</code>ï¼Œ$W^{[2]}$æ˜¯<code class="highlighter-rouge">1x4</code>ï¼Œ$a^{[1]}$æ˜¯<code class="highlighter-rouge">1x1</code>ï¼Œ$b^{[2]}$æ˜¯<code class="highlighter-rouge">1x1</code></p>

<h3 id="forward-propagation">Forward Propagation</h3>

<p>ä¸Šè¿°ç¥ç»ç½‘ç»œåªæœ‰ä¸€ä¸ªç»„è®­ç»ƒé›†ï¼Œå¦‚æœå°†è®­ç»ƒé›†æ‰©å±•åˆ°å¤šç»„($x^{(1)}$,$x^{(2)}$,â€¦,$x^{(m)})$ï¼Œåˆ™æˆ‘ä»¬éœ€è¦ä¸€ä¸ª<code class="highlighter-rouge">for</code>å¾ªç¯æ¥å®ç°æ¯ç»„æ ·æœ¬çš„ç¥ç»ç½‘ç»œè®¡ç®—ï¼Œç„¶åå¯¹å®ƒä»¬è¿›è¡Œæ±‚å’Œ</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& for\ i=1\ to\ m\ \{ \\
& \qquad z^{[1] (i)} = W^{[1] (i)}x^{(i)} + b^{[1]} \\
& \qquad a^{[1] (i)} = \sigma(z^{[1] (i)}) \\ 
& \qquad z^{[2] (i)} = W^{[2]}a^{[1] (i)} + b^{[2]} \\
& \qquad a^{[2] (i)} = \sigma(z^{[2] (i)}) \\
& \}
\end{align*} %]]></script>

<p>ç»“åˆå‰é¢æ–‡ç« å¯çŸ¥ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨å‘é‡åŒ–è®¡ç®—æ¥å–ä»£<code class="highlighter-rouge">for</code>å¾ªç¯ï¼Œå¦</p>

<script type="math/tex; mode=display">% <![CDATA[
X= 
\begin{bmatrix}
. & . & . & . & . \\
| & | & . & . & | \\
x^{(1)} & x^{(2)} & . & . & x^{(m)} \\
| & | & . & . & | \\
. & . & . & . & . \\
\end{bmatrix}
\quad
W^{[1]} = 
\begin{bmatrix}
. & - & -w^{[1]}- & - & . & \\
. & - & -w^{[1]}- & - & . & \\
. & - & -w^{[1]}- & - & . & \\
. & - & -w^{[1]}- & - & . & \\
\end{bmatrix}
\\
A^{[1]} = 
\begin{bmatrix}
. & . & . & . & .  \\
| & | & . & . & | \\
a^{[1] (1)} & a^{[1] (2)} & . & . & a^{[1] (m)} \\
| & | & . & . & | \\
. & . & . & . & .  \\
\end{bmatrix}
\quad
b^{[1]} = 
\begin{bmatrix}
. & . & . & . & .  \\
| & | & . & . & | \\
b^{[1] (1)} & b^{[1] (2)} & . & . & b^{[1] (m)} \\
| & | & . & . & | \\
. & . & . & . & .  \\
\end{bmatrix}
\\
W^{[2]} = 
\begin{bmatrix}
w^{[2]}_1 & w^{[2]}_2  & w^{[2]}_3 & w^{[2]_4}\\
\end{bmatrix}
\\
b^{[2]} = 
\begin{bmatrix}
b^{[2] (1)} & b^{[2] (2)} & ... & b^{[2] (m)}  \\
\end{bmatrix}
\\
A^{[2]} = 
\begin{bmatrix}
a^{[2] (1)} & a^{[2] (2)} & ... & a^{[2] (m)}  \\
\end{bmatrix} %]]></script>

<p>åˆ™ä¸Šè¿°ä¸¤å±‚ç¥ç»ç½‘ç»œçš„å‘é‡åŒ–è¡¨ç¤ºä¸º</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& Z^{[1]} = W^{[1]}X + b^{[1]} \\
& A^{[1]} = \sigma(Z^{[1]}) \\ 
& Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} \\
& A^{[2]} = \sigma(Z^{[2]}) 
\end{align*} %]]></script>

<p>å…¶ä¸­ï¼Œ$X$æ˜¯<code class="highlighter-rouge">3xm</code>, $W^{[1]}$ä¾æ—§æ˜¯<code class="highlighter-rouge">4x3</code>, $A^{[i]}$æ˜¯<code class="highlighter-rouge">4xm</code>ï¼Œ$b^{[1]}$ä¹Ÿæ˜¯<code class="highlighter-rouge">4xm</code>ï¼Œ $W^{[2]}$æ˜¯<code class="highlighter-rouge">3x1</code>ï¼Œ$A^{[2]}$æ˜¯<code class="highlighter-rouge">1xm</code>, $b^{[2]}$æ˜¯<code class="highlighter-rouge">1xm</code>çš„ã€‚ç”±æ­¤å¯ä»¥çœ‹å‡ºï¼Œè®­ç»ƒæ ·æœ¬å¢åŠ å¹¶ä¸å½±å“$W^{[1]}$çš„ç»´åº¦</p>

<h3 id="activation-functions">Activation Functions</h3>

<p>å¦‚æœç¥ç»ç½‘è·¯çš„æŸä¸ªLayerè¦æ±‚è¾“å‡ºç»“æœåœ¨<code class="highlighter-rouge">[0,1]</code>ä¹‹é—´ï¼Œé‚£ä¹ˆé€‰å–$\sigma(x) = \frac{1}{1+e^{-x}}$ä½œä¸ºActivationå‡½æ•°ï¼Œæ­¤å¤–ï¼Œåˆ™å¯ä»¥ä½¿ç”¨<strong>Rectified Linear Unit</strong>å‡½æ•°ï¼š</p>

<script type="math/tex; mode=display">ReLU(z) = g(z) = max(0,z)</script>

<p>å®é™…ä¸Šå¯é€‰æ‹©çš„Activationå‡½æ•°æœ‰å¾ˆå¤šç§ï¼Œä½†å®ƒä»¬éœ€è¦å…·å¤‡ä¸‹é¢çš„æ¡ä»¶</p>

<ol>
  <li>å¿…é¡»æ˜¯éçº¿æ€§çš„</li>
  <li>éœ€è¦å¯å¾®åˆ†ï¼Œå¯è®¡ç®—æ¢¯åº¦</li>
  <li>éœ€è¦æœ‰ä¸€ä¸ªå˜åŒ–sensitiveçš„åŒºåŸŸå’Œä¸€ä¸ªésenstiveçš„åŒºåŸŸ</li>
</ol>

<p>æ€»çš„æ¥è¯´Activationå‡½æ•°çš„ä½œç”¨åœ¨äºé€šè¿‡éçº¿æ€§å˜æ¢ï¼Œè®©ç¥ç»ç½‘ç»œæ˜“äºè®­ç»ƒï¼Œå¯ä»¥æ›´å¥½çš„é€‚åº”æ¢¯åº¦ä¸‹é™</p>

<h3 id="back-propagation">Back Propagation</h3>

<p>ä¸Šè¿°ç¥ç»ç½‘ç»œçš„Costå‡½æ•°å’Œå‰æ–‡ä¸€æ ·</p>

<script type="math/tex; mode=display">J(W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}) = \frac {1}{m} \sum_{i=1}^mL(\hat{y}, y) = \\
- \frac{1}{m} \sum\limits_{i = 1}^{m} \large{(} \small y^{(i)}\log\left(A^{[2] (i)}\right) + (1-y^{(i)})\log\left(1- A^{[2] (i)}\right) \large{)} \small\tag{13}</script>

<p>å…¶ä¸­$Y$ä¸º<code class="highlighter-rouge">1xm</code>çš„è¡Œå‘é‡ $Y = [y^{[1]},y^{[2]},â€¦,y^{[m]}]$ã€‚æŒ‰ç…§ä¸Šä¸€èŠ‚ä»‹ç»çš„é“¾å¼æ±‚å¯¼æ³•åˆ™ï¼Œå¯¹ä¸Šè¿°Costå‡½æ•°æ±‚å¯¼ï¼Œå¯ä»¥å¾—å‡ºä¸‹é¢ç»“è®º(æ¨å¯¼è¿‡ç¨‹çœç•¥)</p>

<ul>
  <li>$dZ^{[2]} = A^{[2]} - Y$</li>
  <li>$dW^{[2]} = \frac{1}{m}dZ^{[2]}A^{[1]^{T}}$</li>
  <li>$db^{[2]} = \frac{1}{m}np.sum(dz^{[2]}, axis=1, keepdims=True)$</li>
  <li>$dz^{[1]} = W^{[2]^{T}}dZ^{[2]} * g^{[1]â€™}(Z^{[1]}) \quad (element-wise \ product)$</li>
  <li>$dW^{[1]} = \frac{1}{m}dZ^{[1]}X^{T}$</li>
  <li>$db^{[1]} = \frac{1}{m}np.sum(dz^{[1]}, axis=1, keepdims=True)$</li>
</ul>

<blockquote>
  <p>å…¶ä¸­$g^{[1]^{â€˜}}(Z^{[1]})$å–å†³äºActivationå‡½æ•°çš„é€‰å–ï¼Œå¦‚æœä½¿ç”¨$tanh$ï¼Œåˆ™$g^{[1]â€™}(Z^{[1]}) = 1-A^{[1]^2}$</p>
</blockquote>

<h3 id="gradient-descent">Gradient Descent</h3>

<p>æœ‰äº†$dW^{[2]}$,$dW^{[1]}$,$db^{[2]}$,$db^{[2]}$çš„è®¡ç®—å…¬å¼ï¼Œæˆ‘ä»¬å˜å¯ä»¥ä½¿ç”¨æ¢¯åº¦ä¸‹é™æ¥æ±‚è§£ $W^{[1]}, b^{[1]}, W^{[2]}, b^{[2]}$äº†ï¼Œå…¶åå‘æ±‚å¯¼çš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º</p>

<p><img src="/assets/images/2018/01/dp-w4-2.png" class="md-img-center" width="80%" /></p>

<p>åœ¨æ¯æ¬¡BPå®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦å¯¹$dw$hå’Œ$db$è¿›è¡Œæ¢¯åº¦ä¸‹é™</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}  \\
& b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} 
\end{align*} %]]></script>

<p>å…¶ä¸­å¯¹$\alpha$çš„å–å€¼éœ€è¦æ³¨æ„ï¼Œä¸åŒlearning rateçš„é€‰å–å¯¹æ¢¯åº¦ä¸‹é™æ”¶æ•›çš„é€Ÿåº¦æœ‰ç€é‡è¦çš„å½±å“ï¼Œå¦‚ä¸‹å›¾</p>

<p><img src="/assets/images/2018/01/dp-w3-4.gif" class="md-img-center" /></p>

<h2 id="resources">Resources</h2>

<ul>
  <li><a href="https://www.coursera.org/specializations/deep-learning">Deep Learning Specialization Course on Coursera</a></li>
  <li><a href="https://livebook.manning.com/book/deep-learning-with-pytorch/welcome/v-10/">Deep Learning with PyTorch</a></li>
</ul>

<h2 id="é™„å½•-numpyå®ç°">é™„å½•: Numpyå®ç°</h2>

<p>æ¥ä¸‹æ¥æˆ‘ä»¬ç”¨numpyæ¥å®ç°ä¸€ä¸ªä¸¤å±‚çš„ç¥ç»ç½‘ç»œï¼Œç¬¬ä¸€å±‚çš„activationå‡½æ•°ä¸ºReluï¼Œç¬¬äºŒå±‚ä¸ºSigmoidã€‚</p>

<ul>
  <li>Initialization</li>
</ul>

<p>ç¬¬ä¸€æ­¥æˆ‘ä»¬æ¥åˆå§‹åŒ–$W$å’Œ$b$ï¼Œæˆ‘ä»¬ä½¿ç”¨<code class="highlighter-rouge">np.random.randn(shape)*0.01</code>æ¥åˆå§‹åŒ–$W$ï¼Œä½¿ç”¨<code class="highlighter-rouge">np.zeros</code>æ¥åˆå§‹åŒ–$b$</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">initialize_parameters</span><span class="p">(</span><span class="n">n_x</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_y</span><span class="p">):</span>
    <span class="s">"""
    Argument:
    n_x -- size of the input layer
    n_h -- size of the hidden layer
    n_y -- size of the output layer
    
    Returns:
    parameters -- python dictionary containing your parameters:
                    W1 -- weight matrix of shape (n_h, n_x)
                    b1 -- bias vector of shape (n_h, 1)
                    W2 -- weight matrix of shape (n_y, n_h)
                    b2 -- bias vector of shape (n_y, 1)
    """</span>
    
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="n">W1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_h</span><span class="p">,</span><span class="n">n_x</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">b1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_h</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">W2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_y</span><span class="p">,</span><span class="n">n_h</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.01</span>
    <span class="n">b2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_y</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span>

    <span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s">"W1"</span><span class="p">:</span> <span class="n">W1</span><span class="p">,</span>
                  <span class="s">"b1"</span><span class="p">:</span> <span class="n">b1</span><span class="p">,</span>
                  <span class="s">"W2"</span><span class="p">:</span> <span class="n">W2</span><span class="p">,</span>
                  <span class="s">"b2"</span><span class="p">:</span> <span class="n">b2</span><span class="p">}</span>
    
    <span class="k">return</span> <span class="n">parameters</span>    
</code></pre></div></div>

<ul>
  <li>Forward Propagation</li>
</ul>

<p>å‚æ•°åˆå§‹åŒ–å®Œæˆåï¼Œæˆ‘ä»¬å¯ä»¥æ¥å®ç°FPäº†ï¼Œå…¶å…¬å¼ä¸º</p>

<script type="math/tex; mode=display">Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\tag{4}</script>

<p>ä¸ºäº†åé¢ä¾¿äºè®¡ç®—back propï¼Œæˆ‘ä»¬ä¼šå°†FPçš„è®¡ç®—ç»“æœç¼“å­˜èµ·æ¥</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">linear_forward</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
    <span class="s">"""
    Implement the linear part of a layer's forward propagation.

    Arguments:
    A -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)

    Returns:
    Z -- the input of the activation function, also called pre-activation parameter 
    cache -- a python tuple containing "A", "W" and "b" ; stored for computing the backward pass efficiently
    """</span>

    <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">A</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span>
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cache</span>

<span class="k">def</span> <span class="nf">linear_activation_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
    <span class="s">"""
    Implement the forward propagation for the LINEAR-&gt;ACTIVATION layer

    Arguments:
    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)
    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)
    b -- bias vector, numpy array of shape (size of the current layer, 1)
    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"

    Returns:
    A -- the output of the activation function, also called the post-activation value 
    cache -- a python tuple containing "linear_cache" and "activation_cache";
             stored for computing the backward pass efficiently
    """</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s">"sigmoid"</span><span class="p">:</span>
        <span class="c1"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".
</span>        <span class="n">Z</span><span class="p">,</span> <span class="n">linear_cache</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s">"relu"</span><span class="p">:</span>
        <span class="c1"># Inputs: "A_prev, W, b". Outputs: "A, activation_cache".
</span>        <span class="n">Z</span><span class="p">,</span> <span class="n">linear_cache</span> <span class="o">=</span> <span class="n">linear_forward</span><span class="p">(</span><span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
        <span class="n">A</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">relu</span><span class="p">(</span><span class="n">Z</span><span class="p">)</span>
    
    <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">linear_cache</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">A</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div>

<ul>
  <li>Cost Function</li>
</ul>

<p>å›é¡¾è®¡ç®—Costå‡½æ•°çš„å…¬å¼å¦‚ä¸‹</p>

<script type="math/tex; mode=display">-\frac{1}{m} \sum\limits_{i = 1}^{m} (y^{(i)}\log\left(a^{[L] (i)}\right) + (1-y^{(i)})\log\left(1- a^{[L](i)}\right)) \tag{7}</script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_cost</span><span class="p">(</span><span class="n">AL</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
    <span class="s">"""
    Implement the cost function defined by equation (7).

    Arguments:
    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)
    Y -- true "label" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)

    Returns:
    cost -- cross-entropy cost
    """</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="c1"># Compute loss from aL and y.
</span>    <span class="n">cost</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">AL</span><span class="o">.</span><span class="n">T</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">Y</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">AL</span><span class="p">)</span><span class="o">.</span><span class="n">T</span><span class="p">))</span>    
    <span class="c1"># To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).
</span>    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>      
    <span class="k">assert</span><span class="p">(</span><span class="n">cost</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="p">())</span>
    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div></div>

<ul>
  <li>Backward propagation</li>
</ul>

<p>å¯¹äºä¸¤å±‚çš„ç¥ç»ç½‘ç»œï¼Œå…¶åå‘æ±‚å¯¼çš„è¿‡ç¨‹å¦‚ä¸‹å›¾æ‰€ç¤º</p>

<p><img src="/assets/images/2018/01/dp-w4-2.png" class="md-img-center" width="80%" /></p>

<p>å¯¹äºç¬¬$l$å±‚ç½‘ç»œï¼ŒFPå¾—åˆ°çš„ç»“æœä¸º$Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$, å‡è®¾æˆ‘ä»¬å·²ç»çŸ¥é“ $dZ^{[l]} = \frac{\partial \mathcal{L} }{\partial Z^{[l]}}$ çš„å€¼ï¼Œæˆ‘ä»¬çš„ç›®çš„æ˜¯æ±‚å‡º $(dW^{[l]}, db^{[l]}, dA^{[l-1]})$ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤º</p>

<p><img src="/assets/images/2018/01/dp-w4-3.png" class="md-img-center" width="50%" /></p>

<p>å…¶ä¸­$dZ^{[l]}$çš„è®¡ç®—å…¬å¼å‰é¢å·²ç»ç»™å‡º</p>

<script type="math/tex; mode=display">dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \tag{11}</script>

<p>numpyå†…ç½®äº†æ±‚è§£<code class="highlighter-rouge">dz</code>çš„å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## sigmoid
</span><span class="n">dZ</span> <span class="o">=</span> <span class="n">sigmoid_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
<span class="c1">## relu
</span><span class="n">dZ</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">activation_cache</span><span class="p">)</span>
</code></pre></div></div>

<p>$dW^{[l]}, db^{[l]}, dA^{[l-1]}$çš„è®¡ç®—å¯å‚è€ƒå‰é¢å°ç»“ç»™å‡ºçš„å…¬å¼</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">linear_backward</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
    <span class="s">"""
    Implement the linear portion of backward propagation for a single layer (layer l)

    Arguments:
    dZ -- Gradient of the cost with respect to the linear output (of current layer l)
    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """</span>
    <span class="n">A_prev</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">cache</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="n">dW</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="n">m</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>
    <span class="n">dA_prev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">dZ</span><span class="p">)</span>
    
    <span class="k">assert</span> <span class="p">(</span><span class="n">dA_prev</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">A_prev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">dW</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span><span class="n">db</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">b</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span>

<span class="k">def</span> <span class="nf">linear_activation_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">cache</span><span class="p">,</span> <span class="n">activation</span><span class="p">):</span>
    <span class="s">"""
    Implement the backward propagation for the LINEAR-&gt;ACTIVATION layer.
    
    Arguments:
    dA -- post-activation gradient for current layer l 
    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently
    activation -- the activation to be used in this layer, stored as a text string: "sigmoid" or "relu"
    
    Returns:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """</span>
    <span class="n">linear_cache</span><span class="p">,</span> <span class="n">activation_cache</span> <span class="o">=</span> <span class="n">cache</span>
    
    <span class="k">if</span> <span class="n">activation</span> <span class="o">==</span> <span class="s">"relu"</span><span class="p">:</span>
        <span class="n">dZ</span> <span class="o">=</span> <span class="n">relu_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span><span class="n">activation_cache</span><span class="p">)</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">linear_backward</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span><span class="n">linear_cache</span><span class="p">)</span>
        
    <span class="k">elif</span> <span class="n">activation</span> <span class="o">==</span> <span class="s">"sigmoid"</span><span class="p">:</span>
        <span class="n">dZ</span> <span class="o">=</span> <span class="n">sigmoid_backward</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span><span class="n">activation_cache</span><span class="p">)</span>
        <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="n">linear_backward</span><span class="p">(</span><span class="n">dZ</span><span class="p">,</span><span class="n">linear_cache</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">dA_prev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div>

<ul>
  <li>Update Parameters</li>
</ul>

<p>åœ¨æ¯æ¬¡BPå®Œæˆåï¼Œæˆ‘ä»¬éœ€è¦å¯¹$dw$hå’Œ$db$è¿›è¡Œæ¢¯åº¦ä¸‹é™</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
& W^{[l]} = W^{[l]} - \alpha \text{ } dW^{[l]}  \\
& b^{[l]} = b^{[l]} - \alpha \text{ } db^{[l]} 
\end{align*} %]]></script>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="k">def</span> <span class="nf">update_parameters</span><span class="p">(</span><span class="n">parameters</span><span class="p">,</span> <span class="n">grads</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">):</span>
    <span class="s">"""
    Update parameters using gradient descent
    
    Arguments:
    parameters -- python dictionary containing your parameters 
    grads -- python dictionary containing your gradients, output of L_model_backward
    
    Returns:
    parameters -- python dictionary containing your updated parameters 
                  parameters["W" + str(l)] = ... 
                  parameters["b" + str(l)] = ...
    """</span>
    
    <span class="n">L</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span> <span class="o">//</span> <span class="mi">2</span> <span class="c1"># number of layers in the neural network
</span>    <span class="c1"># Update rule for each parameter. Use a for loop.
</span>    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">L</span><span class="p">):</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s">"W"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"W"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s">"dW"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
        <span class="n">parameters</span><span class="p">[</span><span class="s">"b"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">[</span><span class="s">"b"</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">grads</span><span class="p">[</span><span class="s">"db"</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
    <span class="k">return</span> <span class="n">parameters</span>
</code></pre></div></div>

:ET