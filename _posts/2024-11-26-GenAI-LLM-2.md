---
list_title: GenAI | Reinforcement learning and LLM-powered applications
title: Reinforcement learning and LLM-powered applications
layout: post
mathjax: true
categories: ["GenAI", "Transformer", "LLM"]
---

## Fine-tuning with human feedback(RLHF)

In 2020, researchers at OpenAI published a paper that explored the use of fine-tuning with human feedback to train a model to write short summaries of text articles

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm3-1.png">

A popular technique to fine-tune large language models with human feedback is called <mark>reinforcement learning from human feedback, or RLHF for short</mark>.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm3-2.png">

## Reinforcement learning (RL) overview

Reinforcement learning is a type of machine learning in which <mark>an agent learns to make decisions related to a specific goal by taking actions in an environment, with the objective of maximizing some notion of a cumulative reward.</mark>

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm3-3.png">

In this framework, the agent continually learns from its experiences by taking actions, observing the resulting changes in the environment, and receiving rewards or penalties, based on the outcomes of its actions. By iterating through this process, the agent gradually refines its strategy or policy to make better decisions and increase its chances of success.
The series of actions taken by the model and corresponding states form a result, often called a <mark>rollout</mark>

## Reinforcement learning: fine-tune LLMs

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm3-4.png">

- In the LLM world, the agent's policy that guides the actions is the LLM, and its objective is to generate text that is perceived as being aligned with the human preferences

- The environment is the context window of the model, the space in which text can be entered via a prompt

- The state that the model considers before taking an action is the current context. That means any text currently contained in the context window

- The action here is the act of generating text. This could be a single word, a sentence, or a longer form text, depending on the task specified by the user. <mark>The sequence of actions and states is called a rollout</mark>

- The action space is the token vocabulary, meaning all the possible tokens that the model can choose from to generate the completion

At any given moment, the action that the model will take, meaning which token it will choose next, <mark>depends on the prompt text in the context and the probability distribution over the vocabulary space</mark>. The reward is assigned based on how closely the completions align with human preferences.

One way you can do this is to have a human evaluate all of the completions of the model against some alignment metric, such as determining whether the generated text is toxic or non-toxic. This feedback can be represented as a scalar value, either a zero or a one. The LLM weights are then updated iteratively to maximize the reward obtained from the human classifier, enabling the model to generate non-toxic completions.

However, obtaining human feedback can be time consuming and expensive. <mark>As a practical and scalable alternative, you can use an additional model, known as the reward model, to classify the outputs of the LLM and evaluate the degree of alignment with human preferences</mark>.

You'll start with a smaller number of human examples to train the secondary model by your traditional supervised learning methods. Once trained, you'll use the reward model to assess the output of the LLM and assign a reward value

<mark>The reward model is the central component of the reinforcement learning process</mark>. It encodes all of the preferences that have been learned from human feedback, and it plays a central role in how the model updates its weights over many iterations
