---
list_title: Note | MIT 6.172 (9) - What Compilers can and cannot Do
title: What Compilers can and cannot Do
layout: post
mathjax: true
---

### Clang/LLVM Compilation Pipeline

This lecture completes more of the story from Lecture 5 about the compilation process

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2021/08/perf-09-01.png">

### Why study the compiler optimizations?

- Compiler can have a big impact on software performance
- Compilers can save you performance-engineering work.
- Compilers help ensure that simple, readable, and maintainable code is fast
- You can understand the differences between the source code and the IR or assembly
- Compilers can make mistakes
- Understanding compilers can help you use them more effectively.

### Simple Model of the Compiler

An optimizing compiler performs a sequence of transformation passes on the code

```shell
LLVM IR -> Transform -> Transform -> Transform -> ... -> Transform -> Optimized LLVM IR
```

- Each transofrmation pass analyzes and edits the code to try to optimize the code's performance
- A transformation pass might run multiple times
- Passes run in a predetermined order that seems to work well most of the time

### Compiler Reports

Clang/LLVM can produce **reports** for many of its transformation passes, not just vectorization:

- `-Rpass=<string>`: Produces reports of which optimizations matching `<string>` were successful.
- `-Rpass-missed<string>`: Produces reports of which optimizations matching `<string>` were not successful.
- `-Rpass-analysis=<string>`: Produces reports of the analyses performed by optimizations matching `<string>`

The argument `<string>` is a regular expression. To see the whole report, use ".*" as the string.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2021/08/perf-09-02.png">

The good news: The compiler can tell you a lot about what's doing

- Many transformation passes in LLVM can report places where they successfully transform code.
- Many can also report the conclusions of their analysis.

The bad news: Reports can be hard to understand.

- The reports can be long and use LLVM jargon.
- Not all transformation passes generate reports.
- Reports don't always tell the whole story.

We want context for understanding these reports


### Compiler Optimizations 

- Data Structures
    - Register allocation
    - Memory to registers
    - Scalar replacement of aggregates
    - Alignment
- Loops
    - Vectorization
    - Unswitching
    - Idiom replacement
    - Loop fission
    - Loop skewing
    - Loop tiling
    - Loop interchange
- Logic
    - Elimination of redundant
    - Memory to registers
    - Strength reduction
    - Dead-code elimination
    - Idiom replacement
    - Branch reordering
    - Global value numbering
- Functions
    - Unswitching
    - Argument elimination

### Optimizing a scalar

Handling one argument, `-O0` code

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2021/08/perf-09-03.png">

- We first allocate some local storage on the stack. 
- Then we store the value from `%2` into that double `a`.
- Later on, we'll load the value out of the `%6` into `%14` before the multiply. 
- Then we load it again before the other multiply.

How do we optimize the code? 

IDEA: Replace the stack - allocated varaiable with the copy in the register. Since the value of `a` is already in the register `%2`, it's not necessary to allocate storage on stack. What we're going to do is just replace those loads from memory with the original argument.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2021/08/perf-09-04.png">

Summary: Compilers transform data structures to store as much as possible in registers.

### Optimizing function calls

Let’s see how compilers optimize function calls. In the first LLVM IR snippet, we have a function call to `vec_scale`, which is the second snippet. In this case, the `vec_scale` is a small function, compiler can just inline that function directly.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2021/08/perf-09-05.png">

This is called function inlining. We identify some function call, or the compiler identifies some function call.And it takes the body of the function, and just pastes it right in place of that call.

SUMMARY: Function inlining and additional transformations can eliminate the cost of the function abstraction.

### Problems with Function Inlining

Why doesn’t the compiler inline all function calls? 

- Some function calls, such as recursive calls <mark>cannot be inlined</mark> execept in sepcial cases, e.g., "recursive tail calls." 
- The compiler cannot inline a function defined in another compilation unit unless one uses <mark>whole-program optimization</mark>.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2021/08/perf-09-06.png">

- Function inlining can <mark>increase code size</mark>, which can hurt performance.



## Resoources

- [Perforamnce Engineering of Software Systems](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-172-performance-engineering-of-software-systems-fall-2018/index.htm)