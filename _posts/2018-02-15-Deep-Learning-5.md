---
list_title: 笔记 | 深度学习 | Optimization Algorithms
title: Optimization Algorithms
layout: post
mathjax: true
categories: ["AI", "Machine Learning","Deep Learning"]
---

## Mini-batch gradient descent

假设一组训练数据$X$有$m$个样本，$X = [x^{(1)},x^{(2)},x^{(3)},...,x^{(m)}]$，每个$x^{[i]}$有n个feature，则$X$矩阵为$(n, m)$，相应的，$Y=[y^{(1)},y^{(2)},y^{(3)},...,y^{(m)}]$，如果$m$很大，即使我们用vectorization的方式也

## Exponentially weighted averages

## Gradient descent with momentum

## Adam optimization algorithm