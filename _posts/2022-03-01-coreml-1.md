---
list_title: On-Device ML | CoreML Essentials
title: Integrating CoreML into PyTorch
layout: post
categories: ["CoreML", "On-Device ML"]
---


### What is CoreML

CoreML is built on top of low-level frameworks such as [Accelerate](https://developer.apple.com/documentation/accelerate), [BNNS](https://developer.apple.com/documentation/accelerate/bnns) and [Metal Performance Shaders](https://developer.apple.com/documentation/metalperformanceshaders)(MPS). 

- Benefits of using CoreML includes:
	- Flexible hardware usage: Depending on the user’s device, CoreML can run the model on the CPU, GPU, or Neural Engine, making optimal use of available hardware.
	- Hybrid execution: The model can be split so that computationally intensive tasks run on the GPU while other parts remain on the CPU.
	- Neural Engine support: On devices with an A12 chip or newer, CoreML can take advantage of the Neural Engine to significantly speed up inference.

- Downsides of using CoreML
	- Limited layer types: Only a narrow set of network layers is officially supported. You can implement custom layers, but this means writing your own ML algorithms—and losing Neural Engine support.
	- Unpredictable performance: Model speed may vary unexpectedly, indicating that CoreML’s scheduling strategies don’t always produce consistent results.
	- Opaque runtime: The CoreML runtime is a "black box", so there’s no guarantee that your model will always run on the Neural Engine.


### The CoreML model format

CoreML models are stored in `.mlmodel` files, which use a `protobuf` format described by the `coremltools` package. The main specification is defined in `Model.proto` and includes:

- Model description: The model’s name, plus its input and output types.
- Model parameters: Parameters that represent a specific instance of the model.
- Metadata: Information about the model’s origin, license, and author.

Although a CoreML model is essentially a protobuf-based binary, `.mlmodel` files must be compiled into an intermediate format before they can run on actual devices.

### Compile CoreML models

There are two ways to compile CoreML models, one is through `coremlc` which is an offline command line tool comes with Xcode. For example, you can run the command below on your macOS machines

```
xcrun coremlc compile ${MODEL_PATH} ${DST_PATH}
```

The command above takes a `.mlmodel` model and generates a `.mlmodelc` folder. The structure is shown below

```
├── analytics
│   └── coremldata.bin
├── coremldata.bin
├── metadata.json
├── model
│   └── coremldata.bin
├── model.espresso.net
├── model.espresso.shape
├── model.espresso.weights
├── model.rank.info.json
└── neural_network_optionals
    └── coremldata.bin
```

Since the internal CoreML stuff is private to developers, there is no document about which file does what. Here are the resources I found
- coremldata.bin: this appears to be the model’s metadata (name of author etc) and the classification labels
- model.espresso.net: describes the structure of the model in JSON format, i.e. which layers it uses and how they are connected to each other
- model.espresso.shape: the output sizes of the layers in the neural network (the same thing you saw in the output of the build step above)
- model.espresso.weights: the learned parameters of the model (this is usually a big file, 12mb for mobilenetv2)
- model/coremldata.bin: ?
“Espresso” is apparently the internal name Apple uses for the part of CoreML that runs neural networks 

The other way to compile CoreML models is through this API at runtime - `compileModelAtURL`. This is basically what TFLite uses to compile their models. In my demo, I also use this API to compile our models. I’ll explain more about this in the delegation APIs section.