---
list_title: GenAI | Instruction Fine-Tuning and Model Evaluation
title: Instruction Fine-Tuning and Model Evaluation
layout: post
mathjax: true
categories: ["GenAI", "Transformer", "LLM"]
---

## Limitations of in-context learning

- May not work for smaller models
- Examples take up space in the context window

## LLM fine-tuning at a high level

In contrast to pre-training, where you train the LLM using vast amounts of unstructured textual data via self-supervised learning, <mark>fine-tuning is a supervised learning process where you use a data set of labeled examples to update the weights of the LLM</mark>. The labeled examples are <mark>prompt completion pairs</mark>, the fine-tuning process extends the training of the model to improve its ability to generate good completions for a specific task.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-1.png">

One strategy, known as <mark>instruction fine-tuning</mark>, is particularly good at improving a model's performance on a variety of tasks.

## Fine Tuning using prompts

Instruction fine-tuning trains the model using examples that demonstrate how it should respond to a specific instruction

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-2.png">

For example, if you want to fine tune your model to improve its summarization ability, you'd build up a data set of examples that begin with the instruction summarize, the following text or a similar phrase. And if you are improving the model's translation skills, your examples would include instructions like translate this sentence.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-3.png">

These prompt completion examples allow the model to learn to generate responses that follow the given instructions.

Instruction fine-tuning, where all of the model's weights are updated is known as <mark>full fine-tuning</mark>. The process results in a new version of the model with updated weights

### The fine-tuning process

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-4.png">

- During fine-tuning, you select prompts from your training data set and pass them to the LLM, which then generates completions.
- Next, you compare the LLM completion with the response specified in the training data. You can see here that the model didn't do a great job, it classified the review as neutral, which is a bit of an understatement. The review is clearly very positive. Remember that the output of an LLM is a probability distribution across tokens. So you can compare the distribution of the completion and that of the training label and use the standard cross entropy function to calculate loss between the two token distributions.
- Then use the calculated loss to update your model weights in standard back propagation. You'll do this for many batches of prompt completion pairs and over several epochs, update the weights so that the model's performance on the task improves
- After you've completed your fine tuning, you can perform a final performance evaluation using the holdout test data set. This will give you the test accuracy.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-5.png">

Fine-tuning with instruction prompts is the most common way to fine-tune LLMs these days. From this point on, when you hear or see the term fine-tuning, you can assume that it always means instruction fine tuning.

### Catastrophic forgetting

While the fine-tuning process allows the model to have a better performance on a single task, it can degrade performance on other tasks. This is called `catastrophic forgetting`.

Catastrophic forgetting happens because the full fine-tuning process modifies the weights of the original LLM.

For example, while fine-tuning can improve the ability of a model to perform sentiment analysis on a review and result in a quality completion, the model may forget how to carry out named entity recognition.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-6.png">
<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-7.png">

How to avoid catastrophic forgetting

- First note that you might not have to!
- Fine-tune on <mark>multiple tasks</mark> at the same time
- Consider <mark>Parameter Efficient Fine-tuning</mark>(PEFT)
  - PEFT is a set of techniques that preserves the weights of the original LLM and trains only a small number of task-specific adapter layers and parameters.

## Multi-task, instruction fine-tuning

Multitask fine-tuning is an extension of single task fine-tuning, where the training dataset is a composed of example inputs and outputs for multiple tasks.

Here, the dataset contains examples that instruct the model to carry out a variety of tasks, including summarization, review rating, code translation, and entity recognition. You train the model on this mixed dataset so that it can improve the performance of the model on all the tasks simultaneously, thus avoiding the issue of catastrophic forgetting

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-8.png">

One drawback to multitask fine-tuning is that it requires a lot of data. You may need as many as 50-100,000 examples in your training set.

## Model Evaluation metrics

In traditional machine learning, you can assess how well a model is doing by looking at its prediction accuracy because the models are deterministic.

$$
\text{Accuracy} = \frac{\text{Correct Predictions}}{\text{Total Predictions}}
$$

But with large language models where the output is non-deterministic and language-based evaluation is much more challenging. We need an automated, structured way to make measurements. `ROUGE` and `BLEU`, are two widely used evaluation metrics for different tasks.

- ROUGE(Recall Oriented Under Study For jesting Evaluation)
  - Used for text summarization
  - Compares a summary to one or more reference summaries
- BLEU SCORE (bilingual evaluation understudy)
  - Used for text translation
  - Compares to human-generated translations

### Terminology

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-9.png">

- A unigram is equivalent to a single word
- A bigram is two words
- A n-gram is a group of n-words

### ROUGE-1

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-10.png">

- The recall metric measures the number of words or unigrams that are matched between the reference and the generated output divided by the number of words or unigrams in the reference.
- Precision measures the unigram matches divided by the output size.
- The F1 score is the harmonic mean of both of these values.

Similarly, `ROUGE-2`, `ROUGE-3` simply uses bigram and n-gram to calculate the corresponding results.

### ROUGE-L

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-11.png">

An alternative way is to use the longest common subsequence present in both the generated output and the reference output.
In this case, the longest matching sub-sequences are, `it is` and `cold outside`, each with a length of two. You can now use the LCS value to calculate the recall precision and F1 score.
