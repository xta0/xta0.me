---
list_title: GenAI | LLM Fine Tuning
title: LLM Fine Tuning
layout: post
categories: ["GenAI", "Transformer", "LLM"]
---

## Limitations of in-context learning

- May not work for samller models
- Examples take up space in the context window

## LLM fine-tuning at a high level

In contrast to pre-training, where you train the LLM using vast amounts of unstructured textual data via self-supervised learning, <mark>fine-tuning is a supervised learning process where you use a data set of labeled examples to update the weights of the LLM</mark>. The labeled examples are <mark>prompt completion pairs</mark>, the fine-tuning process extends the training of the model to improve its ability to generate good completions for a specific task.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-1.png">

One strategy, known as <mark>instruction fine tuning</mark>, is particularly good at improving a model's performance on a variety of tasks.

## Fine Tuning using prompts

Instruction fine-tuning trains the model using examples that demonstrate how it should respond to a specific instruction

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-2.png">

For example, if you want to fine tune your model to improve its summarization ability, you'd build up a data set of examples that begin with the instruction summarize, the following text or a similar phrase. And if you are improving the model's translation skills, your examples would include instructions like translate this sentence.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-3.png">

These prompt completion examples allow the model to learn to generate responses that follow the given instructions.

Instruction fine-tuning, where all of the model's weights are updated is known as <mark>full fine-tuning</mark>. The process results in a new version of the model with updated weights

### The fine-tuning process

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-4.png">

- During fine tuning, you select prompts from your training data set and pass them to the LLM, which then generates completions.
- Next, you compare the LLM completion with the response specified in the training data. You can see here that the model didn't do a great job, it classified the review as neutral, which is a bit of an understatement. The review is clearly very positive. Remember that the output of an LLM is a probability distribution across tokens. So you can compare the distribution of the completion and that of the training label and use the standard crossentropy function to calculate loss between the two token distributions.
- Then use the calculated loss to update your model weights in standard backpropagation. You'll do this for many batches of prompt completion pairs and over several epochs, update the weights so that the model's performance on the task improves
- After you've completed your fine tuning, you can perform a final performance evaluation using the holdout test data set. This will give you the test accuracy.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-5.png">

Fine-tuning with instruction prompts is the most common way to fine-tune LLMs these days. From this point on, when you hear or see the term fine-tuning, you can assume that it always means instruction fine tuning.

### Catastrophic forgetting

While the fine-tuning process allows the model to have a better performance on a single task, it can degrade performance on other tasks. This is called `catastrophic forgetting`.

Catastrophic forgetting happens because the full fine-tuning process modifies the weights of the original LLM.

For example, while fine-tuning can improve the ability of a model to perform sentiment analysis on a review and result in a quality completion, the model may forget how to carry out named entity recognition.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-6.png">
<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm2-7.png">
