---
list_title: GenAI | The Transformer architecture
title: The Transformer architecture
layout: post
categories: ["GenAI", "Transformer"]
---

## How does the Transformer model work at high level

The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence, not just the words next to its neighbors, but every other word in a sentence.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-1.png">

We apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn "who has the book", "who could have the book",

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-2.png">

These attention weights are learned during LLM training. In the end, we will get something called "attention map"

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-3.png">

In this example, you can see that the word book is strongly connected with the word `teacher` and the word `student`.

This is called self-attention and the ability to learn attention in this way across the whole input significantly approves the model's ability to encode language

## The Transformer Architecture

The transformer architecture is split into two distinct parts, the encoder and the decoder.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-4.png">

The inputs are first converted to tokens(one-hot vectors) then passed to the embedding layer. This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. In the original transformer paper, the vector size was actually `512`.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-5.png">

For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words and calcuate the distance between those words.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-6.png">

Since the model processes each of the input tokens **in parallel**. We need to preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. We do so by adding the positional encoding.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-7.png">
