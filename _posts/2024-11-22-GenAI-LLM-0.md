---
list_title: GenAI | An Overview of LLM
title: An Overview of LLM
layout: post
categories: ["GenAI", "Transformer", "LLM"]
---

## Generative models before Transformer

Previous generations of language models made use of an architecture called [RNN](https://xta0.me/2018/05/14/Deep-Learning-12.html). RNNs while powerful for their time, were limited by the amount of compute and memory needed to perform well at generative tasks.

To successfully predict the next word, models need to see more than just the previous few words. Models needs to have an understanding of the whole sentence or even the whole document.

## Attention is all you need

In 2017, after the publication of this paper, "Attention is All You Need", from Google and the University of Toronto. The transformer architecture had arrived. This novel approach unlocked the progress in generative AI that we see today. It can be scaled efficiently to use multi-core GPUs, it can parallel process input data, making use of much larger training datasets, and crucially, it's able to learn to pay attention to the meaning of the words it's processing.

The power of the attenion mechanism lies in its ability to learn the relevance and context of all of the words in a sentence, not just the words next to its neighbors, but every other word in a sentence.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-1.png">

We apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn "who has the book", "who could have the book", and if it's even relevant to the wider context of the document.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-2.png">

These attention weights are learned during LLM training. In the end, we will learn something called "attention map", which can be useful to illustrate the attention weights between each word and every other words.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-3.png">

In this example, you can see that the word book is strongly connected with the word `teacher` and the word `student`.

This is called self-attention and the ability to learn attention in this way across the whole input significantly approves the model's ability to encode language

## The Transformer Architecture Overview

The transformer architecture is split into two distinct parts, the encoder and the decoder. These components work in conjunction with each other and they share a number of similarities.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-4.png">

### Inputs and embedding

The inputs are first converted to tokens(one-hot vectors), with each number representing a position in a dictionary of all the possible words that the model can work with.

> What's important is that once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-27.png">

Now that your input is represented as numbers, you can pass it to the embedding layer. This layer is a trainable vector embedding space, a `high-dimensional space` where each token is represented as a vector and occupies a unique location within that space. The intuition is that these vectors learn to `encode the meaning` and context of individual tokens in the input sequence

In the original transformer paper, the vector size was actually `512`.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-5.png">

For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words and calcuate the distance between those words.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-6.png">

Since the model processes each of the input tokens **in parallel**. We need to preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. We do so by adding the positional encoding.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-7.png">

### Self attention

The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. But this does not happen just once, the transformer architecture actually has `multi-headed self-attention`.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-8.png">

This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.

Each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. While another head may focus on the activity of the sentence.

It is important to note that we don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.

### Feed foward + Softmax layer

The last two layers are feed forward and softmax.They are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here.

## GenAI project lifecycle

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-20.png">

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm3-15.png">

## LLM pre-training at high level

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-22.png">

### Encoder-Only Model

Encoder-only models are also known as Autoencoding models, and they are pre-trained using `masked language modeling`

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-24.png">

Here, tokens in the input sequence are randomly masked, and the training objective is to predict the mask tokens in order to reconstruct the original sentence. This is also called a denoising objective. Autoencoding models spilled bi-directional representations of the input sequence, meaning that the model has an understanding of the full context of a token and not just of the words that come before.

Good use cases:

- Sentiment analysis
- Named entity recognition
- Word classification

Example models:

- BERT
- ROBERTA

### Decoder-Only Model

The decoder-only or autoregressive models are pre-trained using `causal language modeling`. The training objective is to predict the next token based on the previous sequence of tokens.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-23.png">

Decoder-based autoregressive models, mask the input sequence and can only see the input tokens leading up to the token in question. The model has no knowledge of the end of the sentence. The model then iterates over the input sequence one by one to predict the following token.

In contrast to the encoder architecture, the context is unidirectional. By learning to predict the next token from a vast number of examples, the model builds up a statistical representation of language. Models of this type make use of the decoder component off the original architecture without the encoder.

Good use cases:

- Text generation
- Other emergent behavior
  - Depends on the model size

Example models:

- GPT
- BLOOM

### Sequence-to-Sequence Model

The final variation of the transformer model is the sequence-to-sequence model that uses both the encoder and decoder parts off the original transformer architecture.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-25.png">

A popular sequence-to-sequence model T5, pre-trains the encoder using `span corruption`, which masks random sequences of input tokens.

Those mass sequences are then replaced with a unique Sentinel token, shown here as `x`. Sentinel tokens are special tokens added to the vocabulary, but do not correspond to any actual word from the input text.

The decoder is then tasked with reconstructing the mask token sequences auto-regressively. The output is the Sentinel token followed by the predicted tokens.

Good use cases:

- Translation
- Text summarization
  - Question answering

Example models:

- T5
- BART

### Summary

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-26.png">

## Computational Challenges

- Approximate GPU RAM needed to store 1B parameters
  - 1 param (fp32) -> 4 bytes
  - 1B params -> 4GB
- Additional GPU RAM needed to train 1B parameters
  - weights -> +4 bytes per param
  - Adam optimizer -> +8 bytes per param
  - Gradients -> +4 bytes per param
  - Activations and other temp memory -> +8 bytes per param
- In total we will need ~24GB GPU memory (fp32)
- As model sizes get larger, you will need to split your model across multiple GPUs for training

### Quantization

- fp16 (2 bytes)

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-9.png">

- int8 (1 byte)

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-10.png">

- Summary
  - Reduce required memory to store and train models
  - Projects original 32-bit float numbers into lower precision spaces
  - Quantization-aware training(QAT) learns the quantizataion scaling factors during training
  - `bfloat16` is a popular choice

### Multi-GPU Compute Strategies

- Distributed data parallel (DDP)
  - copy the model to each GPU's memory space (if the model can be fit into a single GPU)
  - send batches of data to each GPU in parallel

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-11.png">

DDP copyists your model onto each GPU and sends batches of data to each of the GPUs in parallel. Each data-set is processed in parallel and then a synchronization step combines the results of each GPU, which in turn updates the model on each GPU, which is always identical across chips. This implementation allows parallel computations across all GPUs that results in faster training.

- Fully sharded data parallel (FSDP)
  - Motivated by the "ZeRO" paper - zero data overlap between GPUs
  - Reduce memory by distributing(sharding) the model parameters, gradients, and optimizer states across GPUs
  - Supports offloading to CPU if needed
  - Configure level of sharding via `sharding factor`

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-13.png">

In contrast to DDP, where each GPU has all of the model states required for processing each batch of data available locally, FSDP requires you to collect this data from all of the GPUs before the forward and backward pass

Each CPU requests data from the other GPUs on-demand to materialize the sharded data into uncharted data for the duration of the operation. After the operation, you release the uncharted non-local data back to the other GPUs as original sharded data You can also choose to keep it for future operations during backward pass for example. Note, this requires more GPU RAM again, this is a typical performance versus memory trade-off decision. In the final step after the backward pass, FSDP is synchronizes the gradients across the GPUs in the same way they were for DDP.

## Scaling laws and compute-optimal models

Researchers have explored the relationship between model size, training data size, compute budget and performance in an effort to determine just `how big models need to be`.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-14.png">

- compute budget
- model size
- data size

### Compute budget for training LLMs

Let's first define a unit of compute that quantifies the required resources. A `petaFLOP per second day` is a measurement of the number of floating point operations performed at a rate of one petaFLOP per second, running for an entire day

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-15.png">

When specifically thinking about training transformers, one petaFLOP per second day is approximately equivalent to 8 `NVIDIA V100` GPUs, operating at full efficiency for one full day.

If you have a more powerful processor that can carry out more operations at once, then a petaFLOP per second day requires fewer chips. For example, two `NVIDIA A100` GPUs give equivalent compute to the eight V100 chips.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-16.png">

- Compute budget vs. model performance

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-17.png">

This suggests that you can just increase your compute budget to achieve better model performance. In practice however, the compute resources you have available for training will generally be a hard constraint set by factors such as the hardware you have access to, the time available for training and the financial budget of the project

- Dataset size and model size vs. performance

If we give a fixed value for compute budget. The increase of data size and model size can all lead to a better model performance

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-18.png">

### The Chinchilla paper

There is a [paper](https://arxiv.org/abs/2203.15556) published in 2022. The goal was to find the optimal number of parameters and volume of training data for a given compute budget.

- very large models (e.g. GPT3) may be `over-parameterized` and `under-trained`.
- Smaller models trained on more data could perform as well as large models

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-19.png">

One important takeaway is that the optimal training dataset size for a given model is about `20 times` larger than the number of parameters in the model.

The `BloombergGPT` was trained in a compute optimal way following the Chinchilla loss and so achieves good performance with the size of 50 billion parameters.

## Pre-training for domain adaptation

So far, we've emphasized that you'll generally start with an existing LLM as you develop your application. However, there are domain specific situation where you may find it necessary to pretrain your own model from scratch. For example, legal languages, medical languages.

> "sig: 1 tab po qid pc & hs" meaning Take one tablet by mouth four times a day, after meal and at bedtime

A good example here is the `BloombergGPT` model, which was trained using an extensive financial dataset comprising news articles, reports, and market data, to increase its understanding of finance and enabling it to generate finance-related natural language text.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-21.png">

During the training of BloombergGPT, the authors used the Chinchilla Scaling Laws to guide the number of parameters in the model and the volume of training data, measured in tokens. The recommendations of Chinchilla are represented by the lines `Chinchilla-1`, `Chinchilla-2` and `Chinchilla-3` in the image, and we can see that `BloombergGPT` is close to it.

The recommended configuration was `50 billion` parameters and `1.4` trillion training tokens. However, acquiring 1.4 trillion tokens of training data in the finance domain proved challenging. Consequently, they constructed a dataset containing just `700 billion` tokens, less than the compute-optimal value. Furthermore, due to early stopping, the training process terminated after processing `569 billion` tokens.

The BloombergGPT project is a good illustration of pre-training a model for increased domain-specificity, and the challenges that may force trade-offs against compute-optimal model and training configurations.

## Resources

- [EMNLP: Prompt engineering is the new feature engineering](https://www.amazon.science/blog/emnlp-prompt-engineering-is-the-new-feature-engineering)
- [Attention is All You Need](https://arxiv.org/pdf/1706.03762)
- [BLOOM: BigScience 176B Model](https://arxiv.org/abs/2211.05100)
- [Vector Space Models](https://www.coursera.org/learn/classification-vector-spaces-in-nlp/home/week/3)
- [Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361)
- [What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?](https://arxiv.org/pdf/2204.05832.pdf)
- [LLaMA: Open and Efficient Foundation Language Models](https://arxiv.org/pdf/2302.13971.pdf)
- [Language Models are Few-Shot Learners](https://arxiv.org/pdf/2005.14165.pdf)
- [Training Compute-Optimal Large Language Models](https://arxiv.org/pdf/2203.15556.pdf)
- [BloombergGPT: A Large Language Model for Finance](https://arxiv.org/pdf/2303.17564.pdf)
