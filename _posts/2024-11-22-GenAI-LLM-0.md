---
list_title: LLM Overview
title: LLM Overview
layout: post
categories: ["GenAI", "Transformer", "LLM"]
---

## The Transformer Architecture Overview

The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence, not just the words next to its neighbors, but every other word in a sentence.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-1.png">

We apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn "who has the book", "who could have the book",

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-2.png">

These attention weights are learned during LLM training. In the end, we will get something called "attention map"

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-3.png">

In this example, you can see that the word book is strongly connected with the word `teacher` and the word `student`.

This is called self-attention and the ability to learn attention in this way across the whole input significantly approves the model's ability to encode language

The transformer architecture is split into two distinct parts, the encoder and the decoder.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-4.png">

- Encoder

  - Encodes inputs ("prompts") with contextual understanding and produces one vector per input token

- Decoder
  - Accepts input tokens and generates new tokens

### Inputs and embedding

The inputs are first converted to tokens(one-hot vectors) then passed to the embedding layer. This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. In the original transformer paper, the vector size was actually `512`.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-5.png">

For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words and calcuate the distance between those words.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-6.png">

Since the model processes each of the input tokens **in parallel**. We need to preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. We do so by adding the positional encoding.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-7.png">

### Self attention

The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. But this does not happen just once, the transformer architecture actually has `multi-headed self-attention`.

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-8.png">

This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common.

Each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. While another head may focus on the activity of the sentence.

It is important to note that we don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language.

### Feed foward + Softmax layer

The last two layers are feed forward and softmax.They are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here.

## Model Training

### LLM pre-training at high level

### Computational Challenges

- Approximate GPU RAM needed to store 1B parameters
  - 1 param (fp32) -> 4 bytes
  - 1B params -> 4GB
- Additional GPU RAM needed to train 1B parameters
  - weights -> +4 bytes per param
  - Adam optimizer -> +8 bytes per param
  - Gradients -> +4 bytes per param
  - Activations and other temp memory -> +8 bytes per param
- In total we will need ~24GB GPU memory (fp32)
- As model sizes get larger, you will need to split your model across multiple GPUs for training

### Quantization

- fp16 (2 bytes)

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-9.png">

- int8 (1 byte)

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-10.png">

- Summary
  - Reduce required memory to store and train models
  - Projects original 32-bit float numbers into lower precision spaces
  - Quantization-aware training(QAT) learns the quantizataion scaling factors during training
  - `bfloat16` is a popular choice

### Multi-GPU Compute Strategies

- Distributed data parallel (DDP)
  - copy the model to each GPU's memory space (if the model can be fit into a single GPU)
  - send batches of data to each GPU in parallel

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-11.png">

DDP copyists your model onto each GPU and sends batches of data to each of the GPUs in parallel. Each data-set is processed in parallel and then a synchronization step combines the results of each GPU, which in turn updates the model on each GPU, which is always identical across chips. This implementation allows parallel computations across all GPUs that results in faster training.

- Fully sharded data parallel (FSDP)
  - Motivated by the "ZeRO" paper - zero data overlap between GPUs
  - Reduce memory by distributing(sharding) the model parameters, gradients, and optimizer states across GPUs
  - Supports offloading to CPU if needed

<img class="md-img-center" src="{{site.baseurl}}/assets/images/2024/llm-13.png">

In contrast to DDP, where each GPU has all of the model states required for processing each batch of data available locally, FSDP requires you to collect this data from all of the GPUs before the forward and backward pass

Each CPU requests data from the other GPUs on-demand to materialize the sharded data into uncharted data for the duration of the operation. After the operation, you release the uncharted non-local data back to the other GPUs as original sharded data You can also choose to keep it for future operations during backward pass for example. Note, this requires more GPU RAM again, this is a typical performance versus memory trade-off decision. In the final step after the backward pass, FSDP is synchronizes the gradients across the GPUs in the same way they were for DDP.
